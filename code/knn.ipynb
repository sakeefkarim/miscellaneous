{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM4NI7uXbL/+drzVquASeoi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakeefkarim/miscellaneous/blob/main/code/knn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A *Very* Basic Introduction to $k$-Nearest Neighbours Algorithms\n",
        "\n",
        "[Sakeef M. Karim](https://www.sakeefkarim.com/)\n",
        "\n",
        "sakeef.karim@nyu.edu"
      ],
      "metadata": {
        "id": "A4hkKdfsGme2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries\n",
        "\n",
        "Let's import a few essential libraries (e.g., `pandas` for data wrangling) and submodules (e.g., `sklearn.neighbors` from `scikit-learn`) to develop our $k$-nearest neighbours algorithm."
      ],
      "metadata": {
        "id": "f58X_xV7G2GE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y28BbS74ecTh"
      },
      "outputs": [],
      "source": [
        "# For data manipulation:\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# From scikit-learn, we import modules to pre-process data, fit KNN classifier:\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# numpy will help us set up our Grid Search\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# To save, load our model:\n",
        "\n",
        "from joblib import dump, load"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, we can programmatically mount our Google Drive folders onto a Colab session as follows:"
      ],
      "metadata": {
        "id": "_EjdJuqGHHwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "jBsOGuPKilMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fit a supervised machine learning algorithm, we'll need some labeled data. To this end, we'll once again work with a dataset that we first encountered during the [<font face=\"Inconsolata\" size=4.5> PopAgingDataViz</font>](https://popagingdataviz.com/) workshop —\n",
        " [gapminder](https://jennybc.github.io/gapminder/)."
      ],
      "metadata": {
        "id": "R2a3h6imHqkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading gapminder dataset:\n",
        "\n",
        "gapminder = pd.read_excel(\"https://github.com/sakeefkarim/intro.python.24/raw/main/data/gapminder.xlsx\")"
      ],
      "metadata": {
        "id": "uQJojLjrf7gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the spirit of simplicity, we'll do some ***pre-processing*** by:\n",
        "\n",
        "+ Isolating the latest year in `gapminder` (2007) and dropping the `year` column.\n",
        "\n",
        "+ Generating a dummy indicator (`asia`) of whether a country is in Asia.\n",
        "\n",
        "+ Isolating our feature vector and target variable in separate objects."
      ],
      "metadata": {
        "id": "AkjKhfkKIFgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Homing-in on observations in the latest year (2007)\n",
        "\n",
        "gapminder = gapminder.query(\"year == 2007\").reset_index(drop=True).drop(columns='year')\n",
        "\n",
        "# Generating dummy indicator indexing whether a country is in Asia:\n",
        "\n",
        "gapminder['asia'] = pd.get_dummies(gapminder['continent'])['Asia']\n",
        "\n",
        "# Dropping observations with missing values (not necessary for gapminder):\n",
        "\n",
        "# gapminder.dropna(inplace = True)\n",
        "\n",
        "# Removing target variable and categorical indicators from feature vector:\n",
        "\n",
        "X = gapminder.drop(columns = ['asia', 'continent', 'country'])\n",
        "\n",
        "# Isolating target variable:\n",
        "\n",
        "y = gapminder['asia']"
      ],
      "metadata": {
        "id": "5drCqHyZf7-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split, Cross-Validation\n",
        "\n",
        "Next, we'll split our sample into two disjoint sets: a **training set** featuring 85% of our observations; and a **testing set**—or *hold-out sample* comprising 15% of the original dataset—that will not be involved in the training or validation process. We'll also ensure that our feature vectors have been standardized.\n",
        "\n",
        "Then, we'll initialize our KNN and use (stratified) $k$-fold cross-validation to fit a basic KNN model."
      ],
      "metadata": {
        "id": "Gt2bFqgkuwBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform train-test split:\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,\n",
        "                                                    test_size = 0.15,\n",
        "                                                    random_state = 905)\n",
        "\n",
        "# Standardizing feature data\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Applied to training data\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "\n",
        "# Applied to test data\n",
        "\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Initializing KNN classifier with k = 5, fitting model:\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Stratified k-fold cross-validation:\n",
        "\n",
        "skfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 905)\n",
        "\n",
        "# Cross-validation score:\n",
        "\n",
        "cross_val_score(knn, X_train, y_train, cv = skfold).mean()\n",
        "\n",
        "# Measure of predictive performance:\n",
        "\n",
        "knn.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "qNwwxUnFkaV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization\n",
        "\n",
        "Next, we'll use the `GridSearchCV` method to select the optimal value of $k$ by using a grid search of possible hyperparameter values (odd numbers between 1 and 13)."
      ],
      "metadata": {
        "id": "dihtHnYTV7ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a grid of potential hyperparameter values (odd numbers from 1 to 13):\n",
        "\n",
        "k_grid = {'n_neighbors': np.arange(start = 1, stop = 14, step = 2) }\n",
        "\n",
        "# Setting up a grid search to home-in on best value of k:\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid = k_grid, cv = skfold)\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Extract best score and hyperparameter value:\n",
        "\n",
        "print(\"Best Mean Cross-Validation Score: {:.3f}\".format(grid.best_score_))\n",
        "\n",
        "print(\"Best Parameters (Value of k): {}\".format(grid.best_params_))\n",
        "\n",
        "print(\"Test Set Score: {:.3f}\".format(grid.score(X_test, y_test)))\n",
        "\n",
        "# pd.DataFrame(grid.cv_results_)"
      ],
      "metadata": {
        "id": "eDN5OPERfaid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing Model\n",
        "\n",
        "Finally, we'll generate our $k_3$ model and store it for future use."
      ],
      "metadata": {
        "id": "FD6HRIq2Xp5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting our model of choice:\n",
        "\n",
        "knn_3 = KNeighborsClassifier(n_neighbors = 3)\n",
        "\n",
        "knn_classifier = knn_3.fit(X_train, y_train)\n",
        "\n",
        "# Saving model in Google Drive folder:\n",
        "\n",
        "dump(knn_classifier, '/drive/My Drive/Colab/knn_classifier.joblib')\n",
        "\n",
        "# Using it in the future:\n",
        "\n",
        "# loaded_knn = load('/drive/My Drive/Colab/knn_classifier.joblib')\n",
        "\n",
        "# loaded_knn.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "UTGPvndzYHPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises\n",
        "\n",
        "1. Import the `penguins` data frame from the [`{palmerpenguins}`](https://allisonhorst.github.io/palmerpenguins/) package into Python.\n",
        "\n",
        "2. Isolate observations from the latest `year` in `penguins`.\n",
        "\n",
        "3. Develop a $k$-nearest neighbours **regressor** to predict a numeric outcome of interest. Report your algorithm’s cross-validation score and out-of-sample performance.\n",
        "\n",
        "4.  Develop a $k$-nearest neighbours **classifier** to predict a categorical outcome of interest. Report your algorithm’s cross-validation score and out-of-sample performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "uifqFkJlaOhK"
      }
    }
  ]
}